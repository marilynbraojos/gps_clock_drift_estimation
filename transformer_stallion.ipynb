{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ebb3d81a",
   "metadata": {},
   "source": [
    "# Temporal Fusion Transformer Tutorial for Time Series Forecasting using Pytorch Lightning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9b9d0ce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "mat_input_filepath = \"../gps_datasets/test_transformer/smooth_data_full.mat\"\n",
    "csv_output_path = \"../gps_datasets/test_transformer/combined_data.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "90f77c87",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import os\n",
    "os.environ[\"PYTORCH_ENABLE_MPS_FALLBACK\"] = \"1\"\n",
    "\n",
    "from scipy.io import loadmat\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "\n",
    "import lightning.pytorch as pl \n",
    "from lightning.pytorch.callbacks import EarlyStopping, LearningRateMonitor \n",
    "from lightning.pytorch.loggers import TensorBoardLogger\n",
    "from lightning.pytorch.tuner import Tuner\n",
    "\n",
    "from pytorch_forecasting.models.temporal_fusion_transformer.tuning import optimize_hyperparameters\n",
    "from pytorch_forecasting import Baseline, TemporalFusionTransformer, TimeSeriesDataSet\n",
    "from pytorch_forecasting.data import GroupNormalizer\n",
    "from pytorch_forecasting.metrics import MAE, SMAPE, PoissonLoss, QuantileLoss\n",
    "from pytorch_forecasting.models.temporal_fusion_transformer.tuning import (\n",
    "    optimize_hyperparameters,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8365b7d",
   "metadata": {},
   "source": [
    "## Converting .mat file to .csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e53a56c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "mat = loadmat(mat_input_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "74e36597",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['__header__', '__version__', '__globals__', 'original_correction_value', 'processed_broadcast_clock_bias', 'processed_correction_value', 'None', 'processed_final_clock_bias', '__function_workspace__'])\n"
     ]
    }
   ],
   "source": [
    "print(mat.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c74c2d10",
   "metadata": {},
   "outputs": [],
   "source": [
    "mat = {k: v for k, v in mat.items() if k[0] != '_'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d5e73094",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'original_correction_value': array([[ 3.77025300e-10,  2.07725515e-10,  1.66652731e-10, ...,\n",
      "        -4.01776400e-10, -1.93164357e-10, -3.62395213e-10]]), 'processed_broadcast_clock_bias': array([[-4.46614809e-04, -4.46614687e-04, -4.46614564e-04, ...,\n",
      "        -6.26967326e-05, -6.26965621e-05, -6.26963915e-05]]), 'processed_correction_value': array([[4.17033936e-10, 4.17040428e-10, 4.17053207e-10, ...,\n",
      "        1.30038358e-09, 1.30044855e-09, 1.30050543e-09]]), 'None': MatlabOpaque([(b'processed_epochs', b'MCOS', b'string', array([[3707764736],\n",
      "                     [         2],\n",
      "                     [         1],\n",
      "                     [         1],\n",
      "                     [         1],\n",
      "                     [         1]], dtype=uint32))                          ],\n",
      "             dtype=[('s0', 'O'), ('s1', 'O'), ('s2', 'O'), ('arr', 'O')]), 'processed_final_clock_bias': array([[-4.46614432e-04, -4.46614479e-04, -4.46614397e-04, ...,\n",
      "        -6.26971344e-05, -6.26967552e-05, -6.26967539e-05]])}\n"
     ]
    }
   ],
   "source": [
    "print(mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b1b9ef26",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame({k: pd.Series(v[0]) for k, v in mat.items()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5dcfea00",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv(csv_output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13d4e1bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "221151b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "919ac026",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "960197a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce367dab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "223c28b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Get column names without loading full CSV\n",
    "columns = pd.read_csv('combined_data.csv', nrows=1).columns.tolist()\n",
    "\n",
    "# Efficiently count lines to get shape\n",
    "with open('combined_data.csv') as f:\n",
    "    row_count = sum(1 for _ in f) - 1  # subtract header\n",
    "\n",
    "print(f\"Columns: {columns}\")\n",
    "print(f\"Shape: ({row_count}, {len(columns)})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6738bc01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load only the 'processed_correction_value' column (first 1000 rows)\n",
    "df = pd.read_csv('combined_data.csv', usecols=['processed_correction_value']) # nrows=10000000\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(df.index, df['processed_correction_value'], label='Correction Value')\n",
    "plt.xlabel(\"Index\")\n",
    "plt.ylabel(\"Correction Value\")\n",
    "plt.title(\"Samples of Correction Value\")\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23bf0f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Load only the 'processed_correction_value' column (first 1000 rows)\n",
    "df = pd.read_csv('combined_data.csv', usecols=['processed_broadcast_clock_bias']) # nrows=10000000\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(df.index, df['processed_broadcast_clock_bias'], label='Processed Broadcast Clock Bias Value')\n",
    "plt.xlabel(\"Index\")\n",
    "plt.ylabel(\"Correction Value\")\n",
    "plt.title(\"Samples of Correction Value\")\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0302462",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9271e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Load the full CSV into a DataFrame\n",
    "data = pd.read_csv('combined_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9485524d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.sample(100, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42571873",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbfb2810",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['time_idx'] = range(len(df))\n",
    "data['satellite_id'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18adf727",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.sample(100, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68877b09",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d93b713f",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_prediction_length = 120 # the length of the forecasting \n",
    "max_encoder_length = 240 # how much past data is used to create predictions\n",
    "training_cutoff = data[\"time_idx\"].max() - max_prediction_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "793275f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "training = TimeSeriesDataSet(\n",
    "    data[lambda x: x.time_idx <= training_cutoff],\n",
    "    time_idx=\"time_idx\",\n",
    "    target=\"processed_correction_value\",\n",
    "    group_ids=[\"satellite_id\"],\n",
    "    min_encoder_length=max_encoder_length\n",
    "    // 2,  # keep encoder length long (as it is in the validation set)\n",
    "    max_encoder_length=max_encoder_length,\n",
    "    min_prediction_length=1,\n",
    "    max_prediction_length=max_prediction_length,\n",
    "    static_categoricals=[],\n",
    "    static_reals=[],\n",
    "    time_varying_known_categoricals=[],\n",
    "    time_varying_known_reals=[\"time_idx\"],\n",
    "    time_varying_unknown_categoricals=[],\n",
    "    time_varying_unknown_reals=[\n",
    "        \"processed_broadcast_clock_bias\",\n",
    "        \"processed_final_clock_bias\",\n",
    "        \"processed_correction_value\",\n",
    "    ],\n",
    "    target_normalizer=GroupNormalizer(\n",
    "        groups=[\"satellite_id\"], transformation=\"softplus\"\n",
    "    ),  # use softplus and normalize by group\n",
    "    add_relative_time_idx=True,\n",
    "    add_target_scales=True,\n",
    "    add_encoder_length=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc8b2e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "validation = TimeSeriesDataSet.from_dataset(\n",
    "    training, data, predict=True, stop_randomization=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "352f6664",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataloaders for model\n",
    "batch_size = 128  # set this between 32 to 128\n",
    "train_dataloader = training.to_dataloader(\n",
    "    train=True, batch_size=batch_size, num_workers=0\n",
    ")\n",
    "val_dataloader = validation.to_dataloader(\n",
    "    train=False, batch_size=batch_size * 10, num_workers=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99fba77f",
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_predictions = Baseline().predict(val_dataloader, return_y=True)\n",
    "MAE()(baseline_predictions.output, baseline_predictions.y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3544f26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# configure network and trainer\n",
    "pl.seed_everything(42) # https://pytorch-lightning.readthedocs.io/en/1.7.7/api/pytorch_lightning.utilities.seed.html\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    accelerator=\"cpu\",\n",
    "    gradient_clip_val=0.1,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "283799e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "tft = TemporalFusionTransformer.from_dataset(\n",
    "    training,\n",
    "    # not meaningful for finding the learning rate but otherwise very important\n",
    "    learning_rate=0.03,\n",
    "    hidden_size=8,  # most important hyperparameter apart from learning rate\n",
    "    # number of attention heads. Set to up to 4 for large datasets\n",
    "    attention_head_size=1,\n",
    "    dropout=0.1,  # between 0.1 and 0.3 are good values\n",
    "    hidden_continuous_size=8,  # set to <= hidden_size\n",
    "    loss=QuantileLoss(), # suitable for probabilistic forecasting \n",
    "    optimizer=\"ranger\", # combines the RAdam+ Lookahead - this often leads to a fast convergence\n",
    "    # reduce learning rate if no improvement in validation loss after x epochs\n",
    "    # reduce_on_plateau_patience=1000,\n",
    ")\n",
    "print(f\"Number of parameters in network: {tft.size() / 1e3:.1f}k\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9e0e14b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find optimal learning rate\n",
    "\n",
    "\n",
    "# learning rate tuning to choose an effective learning rate \n",
    "res = Tuner(trainer).lr_find(\n",
    "    tft,\n",
    "    train_dataloaders=train_dataloader,\n",
    "    val_dataloaders=val_dataloader,\n",
    "    max_lr=10.0,\n",
    "    min_lr=1e-6,\n",
    ")\n",
    "\n",
    "print(f\"suggested learning rate: {res.suggestion()}\")\n",
    "fig = res.plot(show=True, suggest=True)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b246442",
   "metadata": {},
   "outputs": [],
   "source": [
    "# configure network and trainer\n",
    "early_stop_callback = EarlyStopping(\n",
    "    monitor=\"val_loss\", min_delta=1e-4, patience=10, verbose=False, mode=\"min\"\n",
    ")\n",
    "lr_logger = LearningRateMonitor()  # log the learning rate\n",
    "logger = TensorBoardLogger(\"lightning_logs\")  # logging results to a tensorboard\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=10, # changed from 50 to 10\n",
    "    accelerator=\"cpu\",\n",
    "    enable_model_summary=True,\n",
    "    gradient_clip_val=0.1,\n",
    "    limit_train_batches=50,  # coment in for training, running valiation every 30 batches\n",
    "    # fast_dev_run=True,  # comment in to check that networkor dataset has no serious bugs\n",
    "    callbacks=[lr_logger, early_stop_callback],\n",
    "    logger=logger,\n",
    ")\n",
    "\n",
    "tft = TemporalFusionTransformer.from_dataset(\n",
    "    training,\n",
    "    learning_rate=0.03,\n",
    "    hidden_size=16,\n",
    "    attention_head_size=2,\n",
    "    dropout=0.1,\n",
    "    hidden_continuous_size=8,\n",
    "    loss=QuantileLoss(),\n",
    "    log_interval=10,  # uncomment for learning rate finder and otherwise, e.g. to 10 for logging every 10 batches\n",
    "    optimizer=\"ranger\",\n",
    "    reduce_on_plateau_patience=4,\n",
    ")\n",
    "print(f\"Number of parameters in network: {tft.size() / 1e3:.1f}k\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56ad39c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit network\n",
    "trainer.fit(\n",
    "    tft,\n",
    "    train_dataloaders=train_dataloader,\n",
    "    val_dataloaders=val_dataloader,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "614be010",
   "metadata": {},
   "source": [
    "# Hyperparameter Tuning \n",
    "\n",
    "Hyperparameter tuning is done with optuna - which is directly built into pytorch-forecasting. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "907fbfb0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eac255fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create study\n",
    "study = optimize_hyperparameters(\n",
    "    train_dataloader,\n",
    "    val_dataloader,\n",
    "    model_path=\"optuna_test\",\n",
    "    n_trials=3,\n",
    "    max_epochs=5,\n",
    "    gradient_clip_val_range=(0.01, 1.0),\n",
    "    hidden_size_range=(8, 128),\n",
    "    hidden_continuous_size_range=(8, 128),\n",
    "    attention_head_size_range=(1, 4),\n",
    "    learning_rate_range=(0.001, 0.1),\n",
    "    dropout_range=(0.1, 0.3),\n",
    "    trainer_kwargs=dict(limit_train_batches=30),\n",
    "    reduce_on_plateau_patience=4,\n",
    "    use_learning_rate_finder=False,  # use Optuna to find ideal learning rate or use in-built learning rate finder\n",
    ")\n",
    "\n",
    "# save study results - also we can resume tuning at a later point in time\n",
    "with open(\"test_study.pkl\", \"wb\") as fout:\n",
    "    pickle.dump(study, fout)\n",
    "\n",
    "# show best hyperparameters\n",
    "print(study.best_trial.params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "637a0bb7",
   "metadata": {},
   "source": [
    "https://www.reddit.com/r/pytorch/comments/1335lwu/pytorch_enable_mps_fallback_help/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4672ea13",
   "metadata": {},
   "source": [
    "# Evaluate Performance \n",
    "\n",
    "PyTorch lightning automatically checkpoints training and therefore we can retrieve the best model and load it. \n",
    "\n",
    "After training, we can make predictions with predict(). This method allows a very fine-grained control over what it returns so that we can easily match the predictions to the pandas dataframe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e839b804",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the best model according to the validation loss\n",
    "# (given that we use early stopping, this is not necessarily the last epoch)\n",
    "best_model_path = trainer.checkpoint_callback.best_model_path\n",
    "best_tft = TemporalFusionTransformer.load_from_checkpoint(best_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "883ba1be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calcualte mean absolute error on validation set\n",
    "predictions = best_tft.predict(\n",
    "    val_dataloader, return_y=True, trainer_kwargs=dict(accelerator=\"cpu\")\n",
    ")\n",
    "MAE()(predictions.output, predictions.y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba342b9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# raw predictions are a dictionary from which all kind of information including quantiles can be extracted\n",
    "raw_predictions = best_tft.predict(\n",
    "    val_dataloader, mode=\"raw\", return_x=True, trainer_kwargs=dict(accelerator=\"cpu\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41bd087b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx in range(10):  # plot 10 examples\n",
    "    best_tft.plot_prediction(\n",
    "        raw_predictions.x, raw_predictions.output, idx=idx, add_loss_to_title=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45ef0b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calcualte metric by which to display\n",
    "predictions = best_tft.predict(\n",
    "    val_dataloader, return_y=True, trainer_kwargs=dict(accelerator=\"cpu\")\n",
    ")\n",
    "mean_losses = SMAPE(reduction=\"none\").loss(predictions.output, predictions.y[0]).mean(1)\n",
    "indices = mean_losses.argsort(descending=True)  # sort losses\n",
    "for idx in range(10):  # plot 10 examples\n",
    "    best_tft.plot_prediction(\n",
    "        raw_predictions.x,\n",
    "        raw_predictions.output,\n",
    "        idx=indices[idx],\n",
    "        add_loss_to_title=SMAPE(quantiles=best_tft.loss.quantiles),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f78c0289",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = best_tft.predict(\n",
    "    val_dataloader, return_x=True, trainer_kwargs=dict(accelerator=\"cpu\")\n",
    ")\n",
    "predictions_vs_actuals = best_tft.calculate_prediction_actual_by_variable(\n",
    "    predictions.x, predictions.output\n",
    ")\n",
    "best_tft.plot_prediction_actual_by_variable(predictions_vs_actuals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0426b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_tft.predict(\n",
    "    training.filter(\n",
    "        lambda x: (x.agency == \"Agency_01\")\n",
    "        & (x.sku == \"SKU_01\")\n",
    "        & (x.time_idx_first_prediction == 15)\n",
    "    ),\n",
    "    mode=\"quantiles\",\n",
    "    trainer_kwargs=dict(accelerator=\"cpu\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "739380cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_prediction = best_tft.predict(\n",
    "    training.filter(\n",
    "        lambda x: (x.agency == \"Agency_01\")\n",
    "        & (x.sku == \"SKU_01\")\n",
    "        & (x.time_idx_first_prediction == 15)\n",
    "    ),\n",
    "    mode=\"raw\",\n",
    "    return_x=True,\n",
    "    trainer_kwargs=dict(accelerator=\"cpu\"),\n",
    ")\n",
    "best_tft.plot_prediction(raw_prediction.x, raw_prediction.output, idx=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b42ed279",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select last 24 months from data (max_encoder_length is 24)\n",
    "encoder_data = data[lambda x: x.time_idx > x.time_idx.max() - max_encoder_length]\n",
    "\n",
    "# select last known data point and create decoder data from it by repeating it and incrementing the month\n",
    "# in a real world dataset, we should not just forward fill the covariates but specify them to account\n",
    "# for changes in special days and prices (which you absolutely should do but we are too lazy here)\n",
    "last_data = data[lambda x: x.time_idx == x.time_idx.max()]\n",
    "decoder_data = pd.concat(\n",
    "    [\n",
    "        last_data.assign(date=lambda x: x.date + pd.offsets.MonthBegin(i))\n",
    "        for i in range(1, max_prediction_length + 1)\n",
    "    ],\n",
    "    ignore_index=True,\n",
    ")\n",
    "\n",
    "# add time index consistent with \"data\"\n",
    "decoder_data[\"time_idx\"] = (\n",
    "    decoder_data[\"date\"].dt.year * 12 + decoder_data[\"date\"].dt.month\n",
    ")\n",
    "decoder_data[\"time_idx\"] += (\n",
    "    encoder_data[\"time_idx\"].max() + 1 - decoder_data[\"time_idx\"].min()\n",
    ")\n",
    "\n",
    "# adjust additional time feature(s)\n",
    "decoder_data[\"month\"] = decoder_data.date.dt.month.astype(str).astype(\n",
    "    \"category\"\n",
    ")  # categories have be strings\n",
    "\n",
    "# combine encoder and decoder data\n",
    "new_prediction_data = pd.concat([encoder_data, decoder_data], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7c551a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_raw_predictions = best_tft.predict(\n",
    "    new_prediction_data,\n",
    "    mode=\"raw\",\n",
    "    return_x=True,\n",
    "    trainer_kwargs=dict(accelerator=\"cpu\"),\n",
    ")\n",
    "\n",
    "for idx in range(10):  # plot 10 examples\n",
    "    best_tft.plot_prediction(\n",
    "        new_raw_predictions.x,\n",
    "        new_raw_predictions.output,\n",
    "        idx=idx,\n",
    "        show_future_observed=False,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7727c1b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "interpretation = best_tft.interpret_output(raw_predictions.output, reduction=\"sum\")\n",
    "best_tft.plot_interpretation(interpretation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66f9ceb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "dependency = best_tft.predict_dependency(\n",
    "    val_dataloader.dataset,\n",
    "    \"discount_in_percent\",\n",
    "    np.linspace(0, 30, 30),\n",
    "    show_progress_bar=True,\n",
    "    mode=\"dataframe\",\n",
    "    trainer_kwargs=dict(accelerator=\"cpu\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4333cada",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting median and 25% and 75% percentile\n",
    "agg_dependency = dependency.groupby(\"discount_in_percent\").normalized_prediction.agg(\n",
    "    median=\"median\", q25=lambda x: x.quantile(0.25), q75=lambda x: x.quantile(0.75)\n",
    ")\n",
    "ax = agg_dependency.plot(y=\"median\")\n",
    "ax.fill_between(agg_dependency.index, agg_dependency.q25, agg_dependency.q75, alpha=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16e6d081",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "toy_transformer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
