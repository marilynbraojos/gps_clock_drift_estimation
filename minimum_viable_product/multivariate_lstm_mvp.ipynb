{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5019b81e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code based off the following tutorial: \n",
    "# https://charlieoneill.medium.com/predicting-the-price-of-bitcoin-with-multivariate-pytorch-lstms-695bc294130"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66ab6f58",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "This is a minimum viable implementation of a learning-based clock bias prediction model. Ideally this model would run locally in a system with limited GPS accessiblity. We have historical IGS data available. \n",
    "\n",
    "Currently in progress is trying the model with many ephemeris features vs just SV name, and past clock bias values reported by IGS. This model is beneficial because IGS normally may take ~17 days to push out very exact clock bias predictions. \n",
    "\n",
    "The objective is to build an AR LSTM with two features: the SV name and its own previous values (i.e. from the previous 20 days to predict the target variable (Final Clock Bias in seconds) for 20 days in the future)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7070c10a",
   "metadata": {},
   "source": [
    "# Data Preprocessing\n",
    "\n",
    "This data is readily available from CDDIS and IGS in the form of RNX files for the satellite ephemeris data and CLK files for the final clock bias that has been processed by IGS and produced after ~17 days. However, if only the AR model is being used then only CLK file data is necessary.\n",
    "\n",
    "Now, the original tutorial is based off financial data, and this has been completed in a different file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76942708",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "import numpy as np \n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import warnings\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import root_mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_percentage_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "451c16f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "historical_window = 5 # [minutes]\n",
    "forward_window = 50 # [minutes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c607d66",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"5_resampled_1min_interval.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b1a7e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(filename, index_col = 'epoch', parse_dates=True)\n",
    "df = df[[\"SV Name\", \"Clock Bias (seconds)\", \"SV Health\", \"File\"]]\n",
    "# df = df[[\"SV Name\", \"Clock Bias (seconds)\", \"Clock Bias\", \"Clock Drift\", \"Clock Drift Rate\", \"SV Health\", \"File\"]]\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97a95c04",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed7288bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(df[(df[\"SV Name\"] == 1)][\"Clock Bias (seconds)\"])\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"Clock Bias (seconds)\")\n",
    "plt.xticks(rotation=90)\n",
    "plt.title(\"Clock Bias for SV1 over time\")\n",
    "plt.savefig(\"sv1_clock_bias_plot.png\", dpi = 250)\n",
    "plt.show(); "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c71b3e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = df.drop(columns=['File']), df['Clock Bias (seconds)'] # in this case, it would be useful to have col names without spaces \n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "654e01d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "mm = MinMaxScaler()\n",
    "ss = StandardScaler()\n",
    "\n",
    "X_trans = ss.fit_transform(X)\n",
    "# y_trans = mm.fit_transform(y.reshape(-1,1)) # reshape has been deprecated https://stackoverflow.com/questions/53723928/attributeerror-series-object-has-no-attribute-reshape \n",
    "y_trans = mm.fit_transform(y.values.reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9e78f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_sequences(input_sequences, output_sequence, n_steps_in, n_steps_out):\n",
    "    X, y = list(), list()\n",
    "    for i in range(len(input_sequences)):\n",
    "        # find the end of the input, output sequence\n",
    "        end_ix = i + n_steps_in\n",
    "        # out_end_ix = end_ix + n_steps_out - 1\n",
    "        out_end_ix = end_ix + n_steps_out\n",
    "\n",
    "        # check if we are beyond the dataset\n",
    "        if out_end_ix > len(input_sequences): break\n",
    "\n",
    "        # gather input and output of the pattern \n",
    "        seq_x, seq_y = input_sequences[i:end_ix], output_sequence[end_ix:out_end_ix, -1]\n",
    "        # seq_x, seq_y = input_sequences[i:end_ix], output_sequence[end_ix:out_end_ix, -1]\n",
    "        X.append(seq_x), y.append(seq_y)\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "X_ss, y_mm = split_sequences(X_trans, y_trans, historical_window, forward_window)\n",
    "print(X_ss.shape, y_mm.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49d25b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_regression_noise(y, std=0.1): \n",
    "    noise = np.random.normal(0, std, size=y.shape)\n",
    "    return y + noise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43a1a927",
   "metadata": {},
   "source": [
    "Let's check if the first sample in <code>y_mm</code> starts at the 100th sample in the original <code>y</code> vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bc6e04e",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert y_mm[0].all() == y_trans[99:149].squeeze(1).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30e31d93",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_mm[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8613e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_trans[99:149].squeeze(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de9dd364",
   "metadata": {},
   "source": [
    "We want to predict the data several months into the future, therefore we can use a training data size of 95% with 5% left for the remaining data for prediction. This gives us a training set of 2763 day (or 7.5 years) and predicting 145 days into the future (5 months). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4537624a",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_samples = len(X)\n",
    "train_test_cutoff = round(0.10 * total_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0edc5a26",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_ss[:-train_test_cutoff]\n",
    "X_test = X_ss[-train_test_cutoff:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3d07249",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = y_mm[:-train_test_cutoff]\n",
    "y_train = add_regression_noise(y_train)\n",
    "y_test = y_mm[-train_test_cutoff:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51b96aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test_cutoff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2c5f354",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training Shape:\", X_train.shape, y_train.shape)\n",
    "print(\"Testing Shape:\", X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5440df4d",
   "metadata": {},
   "source": [
    "Now we convert our data into tensors by calling <code>torch.tensor()</code> on our object, and setting the property <code>requires_grad = True</code>. Some old PyTorch tutorials might indicate that we need to apply <code>Variable</code> on here but this is deprecated. Now the input tensor to be forward propagated has to be can faciliate automatic back propagation through <code> backward()</code> without being wrapped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "034616db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to pytorch tensors \n",
    "X_train_tensors = torch.Tensor(X_train)\n",
    "X_test_tensors = torch.Tensor(X_test)\n",
    "\n",
    "y_train_tensors = torch.Tensor(y_train)\n",
    "y_test_tensors = torch.Tensor(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e06cb075",
   "metadata": {
    "vscode": {
     "languageId": "bat"
    }
   },
   "source": [
    "Looking through the documentation for a multi-layer <code>torch.nn.LSTM</code> shows that the input shape depends on whether the parameter <code>batch_first</code> is true. Since we are accustomed to having the first dimension be the batch, we can set <code>batch_first</code> to true. The size of the input is then <code>(N, L, H_in)</code> where\n",
    "\n",
    "- N is the batch size \n",
    "- L is the sequence length\n",
    "- H_in is the input size (number of features)\n",
    "\n",
    "In other words, we wantt eh dimensions to be the rows of the dataframe in the first dimension. Followed by the length of the dataframe in the next dimension to represent the length of the input sequence, and finally the feature of which we have 4 in the final dimension. \n",
    "\n",
    "To reshape the tensors into our required shape we can use <code>torch.reshape</code>. This takes as arguments the tensors we are reshaping, and then a tuple of the shape we need to reshape to. For the rows of the dataframe, we can simply look at the shape of the first dimension <code>X_train_tensors.shape[0]</code>. Since we are applying an LSTM, we call tha tthe sequence length we feed in is simply 1 because the whole point of the LSTM is not feeding in tons of data at each because the emory is handled by the inner working of the LSTM cell. \n",
    "\n",
    "The last dimension is the number of features which is stored in the <code>X_train_tensors.shape[1]</code>. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1f58cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshape to rows, timestamps, features \n",
    "\n",
    "X_train_tensors_final = torch.reshape(X_train_tensors, \n",
    "                                      (X_train_tensors.shape[0], historical_window, \n",
    "                                      X_train_tensors.shape[2]))\n",
    "\n",
    "X_test_tensors_final = torch.reshape(X_test_tensors, \n",
    "                                     (X_test_tensors.shape[0], historical_window, \n",
    "                                      X_test_tensors.shape[2]))\n",
    "\n",
    "print(\"Training shape:\", X_train_tensors.shape, y_train_tensors.shape)\n",
    "print(\"Testing shape:\", X_test_tensors_final.shape, y_test_tensors.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f7382d2",
   "metadata": {},
   "source": [
    "Let's make sure that the data logic of the test set checks out. Sequential data is hard to get our heads around and especially when generating a test set for multistep output models. Here, we want to take the 100 previous predictors up to the current time step and predict 50 time steps into the future. In the test set, we have 150 batch feature samples, each consisting of 100 time steps and 4 feature predictors. \n",
    "\n",
    "In the targets for the test set, we again have 150 batch samples, each consisting of an array of length 50 of scalar outputs. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2af31754",
   "metadata": {},
   "source": [
    "Since we want a way to validate our results, we need to predict the Bitcoin price for 50 time steps in the test set for which we have the data (i.e. the test targets). Because of the way we wrote the <code> split_sequence()</code>, we just need the last sample of 100 days in <code>X_test</code>, run the model on it and compare those predictions with the last sample of the 50 days of <code>y_test</code>. These correspond to a period of 100 days in the <code>X_test</code>'s last sample, proceeded immediately by the next 50 days in the last sample of <code>y_test</code>. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebab4211",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_check, y_check = split_sequences(X, y.values.reshape(-1,1), historical_window, forward_window) # see above - just doing y.reshape is deprecated\n",
    "X_check[-1][0:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98c23be1",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.iloc[-149:-145]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64414a01",
   "metadata": {},
   "source": [
    "The first four rows of the data match. Note that <code>X_check[-1]</code> should be identical to the <code>X.iloc[-149:-49]</code>, ending 50 days before the end of our dataset. Therefore, we are taking the 100 timesteps of information up to the 26th of Nov 2021, and attempting to predict the 50 days after that up to 14th January 2022. Our final check is to make sure that the final batch sample in our test targets matches these data predictions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26221ef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_check[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1b3245d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Clock Bias (seconds)\"].values[-50:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5815ef9c",
   "metadata": {},
   "source": [
    "Summary: The main performance test for our model will be on the last batch sample in the test set. This will be predictors rom the 100 time steps up to the 26th of Nov 2021 and this will be used by our model to predict the next 50 days of Bitcoin prices up to the 14th of Jan 2022. In this way, we validate model performance by comparing predcitions to the actual prices in that 50 day window. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9be8b75c",
   "metadata": {},
   "source": [
    "# Constructing the LSTM model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51d94ac2",
   "metadata": {},
   "source": [
    "Now, let's construct the LSTM class, inheriting from the nn.Module. In constract to the previous univariate LSTM, we're going to build the model with the nn.LSTM rather than nn.LSTMCell because it's good to understand both options and we dont really need the nn.LSTMCell flexibility right now. We know that nn.LSTM is a recurrent application of the nn.LSTMCell. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d06c2f6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module): \n",
    "    def __init__(self, num_classes, input_size, hidden_size, num_layers): \n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes # output size \n",
    "        self.num_layers = num_layers # number of recurrent layers in the LSTM \n",
    "        self.input_size = input_size # input size\n",
    "        self.hidden_size = hidden_size # neurons in each lstm later\n",
    "\n",
    "        # LSTM model \n",
    "        self.lstm = nn.LSTM(input_size=input_size, hidden_size=hidden_size, num_layers=num_layers, batch_first=True, dropout=0.2)\n",
    "        self.fc1 = nn.Linear(hidden_size, 128) # fully connected \n",
    "        self.fc2 = nn.Linear(128, num_classes) # last FC\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x): \n",
    "        # hidden state\n",
    "        h_0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size)\n",
    "        # cell state\n",
    "        c_0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size)\n",
    "        # propgagate input through LSTM\n",
    "        output, (hn, cn) =self.lstm(x, (h_0, c_0)) # (input, hidden and internal state)\n",
    "        hn = hn.view(-1, self.hidden_size) # reshape data for Dense layer \n",
    "        out = self.relu(hn)\n",
    "        out = self.fc1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1044e312",
   "metadata": {},
   "source": [
    "In the initializations, we start with the parent class, nn.Module. Most initializations in PyTorch are separated into two chunks: \n",
    "\n",
    "- Any wariables that the class will need for reference, for things such as the hidden layer size, input size, and number of layers\n",
    "- Defining the layers of the model without connecting them using the variables instatiated above \n",
    "\n",
    "Frist we need to shape the LSTM with input size, hidden size and number of recurrent layers. For instance num_layers=2 means stacking 2 LSTMs together to form a stacked LSTM with the second LSTM taking in outputs of the first LSTM and computing the final results. Thus we initialize these three variables in the first part of the __init__. We also need to force our model to output only 1 predicted value so we initialize the number of classes variable. \n",
    "\n",
    "In the second part of the __init__, we set out layers of our network with the first later obviously a recurrent aapplication of the LSTM cells, with the parameters specified above: \n",
    "\n",
    "<code> self.lstm = nn.LSTM(input_size=input_size, hidden_size=hidden_size, num_layers=num_layers, batch_first= True)</code>\n",
    "\n",
    "Next, we pass this to a fully connected layer which has an input of the hidden size (the size of the output from the last STM later), and outputs 128 activations. Then we pass these 128 activations to another hidden layer which accepts 128 inputs, and which we want our ouput num_classes (which in this clase would be 1). Finally we pass this activation through a non-linear function, the ReLU. \n",
    "\n",
    "<code>self.fc1 = nn.Linear(hidden_size, 128)</code>\n",
    "\n",
    "\n",
    "<code>self.fc2 = nn.Linear(128, num_classes)</code>\n",
    "\n",
    "\n",
    "<code>self.relu = nn.ReLU </code>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d51ba428",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "106fa0ea",
   "metadata": {},
   "source": [
    "Here we will use a typical regression loss function like the MSE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "addac98a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(n_epochs, lstm, optimiser, loss_fn, X_train, y_train, X_test, y_test):\n",
    "    for epoch in range(n_epochs): \n",
    "        lstm.train()\n",
    "        outputs = lstm.forward(X_train) # forward pass\n",
    "        optimiser.zero_grad() # calculate the gradient by manually setting to 0 \n",
    "\n",
    "        # obtain loss function \n",
    "        loss = loss_fn(outputs, y_train)\n",
    "        loss.backward() # calculates the loss of the loss function \n",
    "        optimiser.step() # improve the loss, i.e. backprop \n",
    "\n",
    "        # test loss\n",
    "        lstm.eval()\n",
    "        test_preds = lstm(X_test)\n",
    "        test_loss = loss_fn(test_preds, y_test)\n",
    "\n",
    "        if epoch % 2 == 0: \n",
    "            print(\"Epoch: %d, train loss: %1.5f, test loss: %1.5f\" % (epoch, loss.item(), test_loss.item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "812b2410",
   "metadata": {},
   "source": [
    "Here, we are doing the forward through the network by passing the training tensors, which we shaped before. We then zero out the current gradients in the torch computational graph. We compare these outputs from the forward pass with the actual train targets using our loss function, and backpropagate with loss.backward() to calculate the gradients of the loss wrt to the parameters (the weights and biases). We then use this loss to update the parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc7ea564",
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "n_epochs = 100\n",
    "learning_rate = 0.001 \n",
    "\n",
    "input_size = X_ss.shape[2] # num of features\n",
    "hidden_size = 2 # num of features in the hidden state\n",
    "num_layers = 1\n",
    "num_classes = forward_window # num of output classes\n",
    "\n",
    "lstm = LSTM(num_classes, input_size, hidden_size, num_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd9f82a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = torch.nn.MSELoss()\n",
    "optimiser = torch.optim.Adam(lstm.parameters(), lr = learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1862b3c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_loop(n_epochs=n_epochs, \n",
    "              lstm=lstm, \n",
    "              optimiser=optimiser, \n",
    "              loss_fn = loss_fn, \n",
    "              X_train=X_train_tensors_final, \n",
    "              y_train=y_train_tensors, \n",
    "              X_test=X_test_tensors_final, \n",
    "              y_test=y_test_tensors)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfea4699",
   "metadata": {},
   "source": [
    "# Prediction\n",
    "\n",
    "A key part of prediction, if the variables have been standardized, is known was we need to pass to the model. That is what do we need to standardize before passing it to hte model. We just apply the same transforms to the predictors and targets so that whatever we feed to the model is exactly what the model is seeing. It wouldnt make sense to not stardize our inpits; the model would be confused as to why they no longer have zero mean and unit variance. \n",
    "\n",
    "Let's plot the result for the full dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8b41188",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_X_ss = ss.transform(df.drop(columns=['File'])) # old transformers \n",
    "df_y_mm = mm.transform(df[\"Clock Bias (seconds)\"].values.reshape(-1,1)) # older transformers\n",
    "\n",
    "#split sequences \n",
    "df_X_ss, df_y_mm = split_sequences(df_X_ss, df_y_mm, historical_window, forward_window)\n",
    "\n",
    "# convert tensors \n",
    "df_X_ss = torch.Tensor(df_X_ss)\n",
    "df_y_mm = torch.Tensor(df_y_mm)\n",
    "\n",
    "#reshape dataset\n",
    "df_X_ss = torch.reshape(df_X_ss, (df_X_ss.shape[0], historical_window, df_X_ss.shape[2]))\n",
    "\n",
    "train_predict = lstm(df_X_ss)\n",
    "data_predict = train_predict.data.numpy()\n",
    "dataY_plot = df_y_mm.data.numpy()\n",
    "\n",
    "data_predict = mm.inverse_transform(data_predict)\n",
    "dataY_plot = mm.inverse_transform(dataY_plot)\n",
    "\n",
    "true, preds = [], []\n",
    "\n",
    "for i in range(len(dataY_plot)): \n",
    "    true.append(dataY_plot[i][0])\n",
    "for i in range(len(data_predict)): \n",
    "    preds.append(data_predict[i][0])\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "test_cutoff = round(0.90 * total_samples)\n",
    "plt.axvline(x=test_cutoff, c='r', linestyle='--') # size of the training set\n",
    "\n",
    "plt.plot(true, label='Actual Data')\n",
    "plt.plot(preds, label='Predicted Data')\n",
    "plt.title('Time Series Prediction')\n",
    "plt.legend()\n",
    "plt.savefig(\"whole_plot.png\", dpi = 300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ed06088",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c2fc44b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f429e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "r2 = r2_score(true[test_cutoff:], preds[test_cutoff:])\n",
    "r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25860789",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_absolute_percentage_error(true, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa9b50ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "root_mean_squared_error(true, preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caca5958",
   "metadata": {},
   "source": [
    "Although these results may seem good, it's not actually as good as it appears. This plot above, where the test data is any time step beyond the red-dashed line, seems to suggest that our moel is amazingly accurate at predicting the price of Bitcoin, a few months into the future. However, what's happening here is a form of data leakage, where information about the test targets has leaked into the test features. That is, whenever we run the model, it has access to the <code>Open</code> price for that day, which is obviously going to be extremely close to the price Bitcoin endsup at for the particular time step. When we loop over our predictions on the model, we append the first predicted value for that time-step. This means that every prediction we are plotting on here has the benefit of being the next predicted value - and not the sequence of 50 values into the future (as we set up in our problem definition). \n",
    "\n",
    "\n",
    "If we really want to see how our model is performing, we have to feed it the 100 time-steps of features before the final value in the test set and then use one model forward pass to calculate the 50 time-step prices in the test set. \n",
    "\n",
    "Recall from above that double-checked our test features in the final batch sample in the test set werre the 100 time steps up to the time-step we're attempting to predict. That is, we feed the model 100 days of information upt to 26th Nov 2021, and get it to predict 50 days of Bitcoin prices, from this day to the 14th Jan 2022. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a05310e",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predict = lstm(X_test_tensors_final[-1].unsqueeze(0)) # obtains the last sample\n",
    "test_predict = test_predict.detach().numpy()\n",
    "test_predict = mm.inverse_transform(test_predict)\n",
    "test_predict = test_predict[0].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8393b126",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_target = y_test_tensors[-1].detach().numpy()\n",
    "test_target = mm.inverse_transform(test_target.reshape(1,-1))\n",
    "test_target = test_target[0].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0558c254",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(test_target, label = \"Actual Data\")\n",
    "plt.plot(test_predict, label = \"LSTM Predictions\")\n",
    "plt.savefig(\"small_plot.png\", dpi = 300)\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98e8e3d1",
   "metadata": {},
   "source": [
    "This is good. If we feed the last 100 days of information, our model successfully predicts a steady decline in the price of Bitcoin over the next 50 days. For one last plot, let's put this in perspective of the scale of the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09353f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,6))\n",
    "a = [x for x in range(2500, len(y))]\n",
    "plt.plot(a, y[2500:], label=\"Actual Data\"); \n",
    "c = [x for x in range (len(y)-forward_window, len(y))]\n",
    "plt.plot(c, test_predict, label = \"One-shot multi-step prediction (50 days)\")\n",
    "plt.axvline(x=len(y)-forward_window, c='r', linestyle=\"--\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37ce39bc",
   "metadata": {},
   "source": [
    "So, if we'd run our model on 26 Nov 2021, we would have been correct in selling off our Bitcoin. The model correctly predicts a price drop, as well as the rate at which it drops. \n",
    "\n",
    "This orange whole curve is generated without looking at ANY target data. Yes, to be fair, the model is very familiar with all the actual targets before the dashed red line. However, once we move past the training set into the test set the model has no idea what the test targets are. It has to generate its prediction  from the input feature only. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bde103c2",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc05dfb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "r2 = r2_score(test_target, test_predict)\n",
    "r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8be8ad20",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c90c81c",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85c2dab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "root_mean_squared_error(test_target, test_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b61ef9c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e25cdebd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9c6573b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9da92f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "985f4e96",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "toy_transformer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
